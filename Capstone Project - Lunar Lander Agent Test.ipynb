{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Testing with OpenAI Gym's Continuous Lunar Lander Environment ##\n",
    "\n",
    "In order to ensure that the agents built for this project were working correctly, they were first tested on OpenAI Gym's continuous lunar lander environment. Continuous lunar lander is a difficult task, but it is solvable. The environment is considered solved when an agent gets an average score of >=200 over 100 episodes. \n",
    "\n",
    "We won't go into too much detail on the agents here, but we'll do a brief walkthrough of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import sys\n",
    "\n",
    "from simplerl import (DDPGAgent, \n",
    "                      DDPGNet,\n",
    "                      TD3Agent,\n",
    "                      TD3Net,\n",
    "                      OrnsteinUhlenbeckProcess, \n",
    "                      GaussianProcess,\n",
    "                      ExponentialScheduler, \n",
    "                      ConstantScheduler, \n",
    "                      LinearScheduler,\n",
    "                      train,\n",
    "                      GymMonitorHook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first agent built was a DDPG agent. You can read about the agent design in detail here: https://arxiv.org/pdf/1509.02971. For this task, we'll use a simple two layer feed forward network with 256 units in the first layer and 128 units in the second layer for both the policy and critic networks; there will be no shared parameters between the policy and critic networks. We'll also use the exploration noise suggested in the paper, an Ornstein Uhlenbeck process. \n",
    "\n",
    "The full list of selected hyperparameters are below. This first version of DDPG is the basic implementation, using regular experience replay, 1-step returns, and a single actor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_fn = lambda: gym.make('LunarLanderContinuous-v2')\n",
    "model_fn = lambda: DDPGNet(8, 2, (256, 128), action_scale = 1.0)\n",
    "noise_fn = lambda: OrnsteinUhlenbeckProcess((2, ), ConstantScheduler(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50 | Time Steps: 86 | Average Score: -246.59\n",
      "Episode 100 | Time Steps: 107 | Average Score: -266.01\n",
      "Episode 150 | Time Steps: 190 | Average Score: -335.07\n",
      "Episode 200 | Time Steps: 222 | Average Score: -338.35\n",
      "Episode 250 | Time Steps: 280 | Average Score: -224.36\n",
      "Episode 300 | Time Steps: 1000 | Average Score: -124.28\n",
      "Episode 350 | Time Steps: 1000 | Average Score: -30.89\n",
      "Episode 400 | Time Steps: 1000 | Average Score: -27.26\n",
      "Episode 450 | Time Steps: 627 | Average Score: -32.18\n",
      "Episode 500 | Time Steps: 479 | Average Score: 70.16\n",
      "Episode 550 | Time Steps: 322 | Average Score: 101.76\n",
      "Episode 600 | Time Steps: 571 | Average Score: 91.79\n",
      "Episode 650 | Time Steps: 1000 | Average Score: 71.11\n",
      "Episode 700 | Time Steps: 1000 | Average Score: 9.55\n",
      "Episode 750 | Time Steps: 1000 | Average Score: 32.12\n",
      "Episode 800 | Time Steps: 1000 | Average Score: 72.19\n",
      "Episode 821 | Time Steps: 654 | Average Score: 90.28\n",
      "\n",
      "Wall time: 43.61\n"
     ]
    }
   ],
   "source": [
    "agent = DDPGAgent(env_fn = env_fn, \n",
    "                  model_fn = model_fn,\n",
    "                  n_actors = 1,\n",
    "                  action_range = 1.0,\n",
    "                  gamma = 0.99,\n",
    "                  exploration_noise = noise_fn,\n",
    "                  batch_size = 64,\n",
    "                  n_steps = 1,\n",
    "                  replay_memory = 1000000,\n",
    "                  use_per = False,\n",
    "                  replay_start = 1000,\n",
    "                  parameter_tau = 1e-3,\n",
    "                  buffer_tau = 1e-3,\n",
    "                  optimizer = optim.Adam,\n",
    "                  policy_learning_rate = 1e-4,\n",
    "                  critic_learning_rate = 1e-3,\n",
    "                  weight_decay = 1e-5,\n",
    "                  clip_gradients = 5.0,\n",
    "                  update_freq = 4)\n",
    "\n",
    "monitor = GymMonitorHook(verbose = 50, vector_env = True)\n",
    "agent.train()\n",
    "t = time.time()\n",
    "train(agent, agent.env, train_steps = 400000, hooks = [monitor], vector_env = True)\n",
    "print('\\nWall time: {:.2f}'.format((time.time() - t) / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent learns, but it is unable to solve the environment in the given 400,000 steps. With more training, vanilla DDPG is able to solve this environment though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use a more sophisticated version of DDPG. This version uses:\n",
    "1. Prioritized experience replay to better select experiences to learn from, which you can read about in detail here: https://arxiv.org/pdf/1511.05952.\n",
    "2. 5-step returns to reduce bias, at the cost of somewhat increasing variance, of the value estimates.\n",
    "3. 16 parallel actors (implemented using Python's multiprocessing library), to increase exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50 | Time Steps: 139 | Average Score: -171.67\n",
      "Episode 100 | Time Steps: 275 | Average Score: -186.51\n",
      "Episode 150 | Time Steps: 580 | Average Score: -148.35\n",
      "Episode 200 | Time Steps: 959 | Average Score: -8.21\n",
      "Episode 250 | Time Steps: 687 | Average Score: 107.41\n",
      "Episode 300 | Time Steps: 323 | Average Score: 153.65\n",
      "Episode 350 | Time Steps: 263 | Average Score: 170.48\n",
      "Episode 400 | Time Steps: 360 | Average Score: 190.53\n",
      "Episode 450 | Time Steps: 284 | Average Score: 216.16\n",
      "Episode 500 | Time Steps: 529 | Average Score: 222.36\n",
      "Episode 503 | Time Steps: 597 | Average Score: 221.50\n",
      "\n",
      "Wall time: 61.01\n"
     ]
    }
   ],
   "source": [
    "agent = DDPGAgent(env_fn = env_fn, \n",
    "                  model_fn = model_fn,\n",
    "                  n_actors = 16,\n",
    "                  action_range = 1.0,\n",
    "                  gamma = 0.99,\n",
    "                  exploration_noise = noise_fn,\n",
    "                  batch_size = 64,\n",
    "                  n_steps = 5,\n",
    "                  replay_memory = 1000000,\n",
    "                  use_per = True,\n",
    "                  alpha = 0.6,\n",
    "                  beta = LinearScheduler(0.4, 1.0, 100000),\n",
    "                  replay_start = 1000,\n",
    "                  parameter_tau = 1e-3,\n",
    "                  buffer_tau = 1e-3,\n",
    "                  optimizer = optim.Adam,\n",
    "                  policy_learning_rate = 1e-4,\n",
    "                  critic_learning_rate = 1e-3,\n",
    "                  weight_decay = 1e-5,\n",
    "                  clip_gradients = 5.0,\n",
    "                  update_freq = 4)\n",
    "\n",
    "monitor = GymMonitorHook(verbose = 50, vector_env = True)\n",
    "agent.train()\n",
    "t = time.time()\n",
    "train(agent, agent.env, train_steps = 200000, hooks = [monitor], vector_env = True)\n",
    "print('\\nWall time: {:.2f}'.format((time.time() - t) / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This agent is able to quickly solve the environment in about 400 episodes, and learning is much more stable with these extra features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we'll use a TD3 agent. TD3 is a variant of DDPG with 3 main differences:\n",
    "1. The policy network is updated less frequently than the critic network so that the critic can provide better value estimates for the policy gradient. Here, the critic is updated twice before the policy network is updated.\n",
    "2. Two critic networks are used to reduce overestimation bias in the value estimates. This is similar to double-Q learning in traditional reinforcement learning and Double DQN.\n",
    "3. Gaussian noise is added to the actions passed to the critic networks as a form of regularization during training.\n",
    "\n",
    "You can read more about TD3 here: https://arxiv.org/pdf/1802.09477."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50 | Time Steps: 164 | Average Score: -282.23\n",
      "Episode 100 | Time Steps: 707 | Average Score: -271.86\n",
      "Episode 150 | Time Steps: 1000 | Average Score: -206.34\n",
      "Episode 200 | Time Steps: 504 | Average Score: -97.24\n",
      "Episode 250 | Time Steps: 662 | Average Score: 52.56\n",
      "Episode 300 | Time Steps: 690 | Average Score: 182.74\n",
      "Episode 350 | Time Steps: 181 | Average Score: 213.09\n",
      "Episode 400 | Time Steps: 337 | Average Score: 212.59\n",
      "Episode 450 | Time Steps: 153 | Average Score: 219.05\n",
      "Episode 500 | Time Steps: 213 | Average Score: 224.59\n",
      "Episode 550 | Time Steps: 202 | Average Score: 230.64\n",
      "Episode 574 | Time Steps: 142 | Average Score: 240.74\n",
      "\n",
      "Wall time: 81.87\n"
     ]
    }
   ],
   "source": [
    "model_fn = lambda: TD3Net(8, 2, (256, 128), action_scale = 1.0)\n",
    "\n",
    "agent = TD3Agent(env_fn = env_fn, \n",
    "                 model_fn = model_fn,\n",
    "                 n_actors = 16,\n",
    "                 action_range = 1.0,\n",
    "                 gamma = 0.99,\n",
    "                 exploration_noise = noise_fn,\n",
    "                 regularization_noise = GaussianProcess((64, 2), ConstantScheduler(0.2)),\n",
    "                 noise_clip = (-0.5, 0.5),\n",
    "                 batch_size = 64,\n",
    "                 n_steps = 5,\n",
    "                 replay_memory = 100000,\n",
    "                 use_per = True,\n",
    "                 alpha = 0.6,\n",
    "                 beta = LinearScheduler(0.4, 1.0, 100000),\n",
    "                 replay_start = 1000,\n",
    "                 parameter_tau = 1e-3,\n",
    "                 buffer_tau = 1e-3,\n",
    "                 optimizer = optim.Adam,\n",
    "                 policy_learning_rate = 1e-4,\n",
    "                 critic_learning_rate = 1e-3,\n",
    "                 weight_decay = 1e-5,\n",
    "                 clip_gradients = 5.0,\n",
    "                 update_freq = 4, \n",
    "                 policy_update_freq = 8)\n",
    "\n",
    "monitor = GymMonitorHook(verbose = 50, vector_env = True)\n",
    "agent.train()\n",
    "t = time.time()\n",
    "train(agent, agent.env, train_steps = 200000, hooks = [monitor], vector_env = True)\n",
    "print('\\nWall time: {:.2f}'.format((time.time() - t) / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TD3 agent performs the best, solving the environment in about 350 espisodes, about 100 less than DDPG. While more complicated than DDPG, the improvements go a long way to stabilizing learning, and, therefore, we used TD3 as the final agent for the project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep Learning Env",
   "language": "python",
   "name": "deep_learning_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
